{"cells":[{"cell_type":"markdown","metadata":{"id":"13Ewpcs3Nv9t"},"source":["#Lab 3: Pretrained Networks\n","\n","---\n","In this lab, we will explore two popular pretrained models: a model that can label an image according to its content, and another that can fabricate a new image from a real image (image-to-image translation)."]},{"cell_type":"markdown","metadata":{"id":"Ptb1cxRBXzNS"},"source":["## Part 1: A pretrained network that recognizes the subject of an image<br>\n","We’ll run a state-of-the-art deep neural network that was pretrained on an object-recognition task.<br> \n","<br>\n","The pretrained network we’ll explore here was trained on a subset of the ImageNet dataset (http://imagenet.stanford.edu).<br>\n","ImageNet is a very large dataset of over 14 million images maintained by Stanford University. All of the images are labeled with a hierarchy of nouns that come from the WordNet dataset (http://wordnet.princeton.edu).<br>\n","<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ONBz2k4U0g8l"},"outputs":[],"source":["# Mounting Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3LPDzJcw7VI"},"outputs":[],"source":["# Import libraries\n","from torchvision import models\n","import torch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sRKJcQ_w_Hj"},"outputs":[],"source":["# Directory function to examine the models\n","dir(models)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1Z6JM9VwVIQ"},"outputs":[],"source":["# Instantiate a 101-layer convolutional neural network. (it downloads the weights of resnet101 trained on the ImageNet dataset, with 1.2 million images and 1,000 categories)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kp54LDNyyH5i"},"outputs":[],"source":["# Architecture of resnet\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ZtCnfC4ynOv"},"outputs":[],"source":["# Loading an image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lVsJ4DOEw8tT"},"outputs":[],"source":["# Preprocess the images: scale the input image to 256 × 256, crop the image to 224 × 224 around the center, transform it to a tensor, and normalize its RGB (red, green, blue) components to match what was presented to the network during training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VxtQyinJ05Ud"},"outputs":[],"source":["# Pass the image through our preprocessing pipeline\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OapN_DQL1IU4"},"outputs":[],"source":["# Predict image class\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dZub4Rlm2W8Z"},"outputs":[],"source":["# Find out the label of the class that received the highest score: load a text file listing the labels in the same order they were presented to the network during training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ev4CqEon3WnH"},"outputs":[],"source":["# Using the max function in PyTorch, which outputs the maximum value in a tensor as well as the indices where that maximum value occurred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wryxh7Px3f3N"},"outputs":[],"source":["# Sorts the values\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kfzrdQ7o8PvQ"},"outputs":[],"source":["# Load another image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MSdxJQfO8iiT"},"outputs":[],"source":["# Process the image and predict the class\n"]},{"cell_type":"markdown","metadata":{"id":"uERzLSL2-QUl"},"source":["## Part 2: A pretrained model that fakes it until it makes it<br>\n","The CycleGAN network has been trained on a dataset of (unrelated) horse images and zebra images extracted from the ImageNet dataset.<br> "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hu_CJedGnqb"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","class ResNetBlock(nn.Module):\n","\n","    def __init__(self, dim):\n","        super(ResNetBlock, self).__init__()\n","        self.conv_block = self.build_conv_block(dim)\n","\n","    def build_conv_block(self, dim):\n","        conv_block = []\n","\n","        conv_block += [nn.ReflectionPad2d(1)]\n","\n","        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n","                       nn.InstanceNorm2d(dim),\n","                       nn.ReLU(True)]\n","\n","        conv_block += [nn.ReflectionPad2d(1)]\n","\n","        conv_block += [nn.Conv2d(dim, dim, kernel_size=3, padding=0, bias=True),\n","                       nn.InstanceNorm2d(dim)]\n","\n","        return nn.Sequential(*conv_block)\n","\n","    def forward(self, x):\n","        out = x + self.conv_block(x) \n","        return out\n","\n","\n","class ResNetGenerator(nn.Module):\n","\n","    def __init__(self, input_nc=3, output_nc=3, ngf=64, n_blocks=9): \n","\n","        assert(n_blocks >= 0)\n","        super(ResNetGenerator, self).__init__()\n","\n","        self.input_nc = input_nc\n","        self.output_nc = output_nc\n","        self.ngf = ngf\n","\n","        model = [nn.ReflectionPad2d(3),\n","                 nn.Conv2d(input_nc, ngf, kernel_size=7, padding=0, bias=True),\n","                 nn.InstanceNorm2d(ngf),\n","                 nn.ReLU(True)]\n","\n","        n_downsampling = 2\n","        for i in range(n_downsampling):\n","            mult = 2**i\n","            model += [nn.Conv2d(ngf * mult, ngf * mult * 2, kernel_size=3,\n","                                stride=2, padding=1, bias=True),\n","                      nn.InstanceNorm2d(ngf * mult * 2),\n","                      nn.ReLU(True)]\n","\n","        mult = 2**n_downsampling\n","        for i in range(n_blocks):\n","            model += [ResNetBlock(ngf * mult)]\n","\n","        for i in range(n_downsampling):\n","            mult = 2**(n_downsampling - i)\n","            model += [nn.ConvTranspose2d(ngf * mult, int(ngf * mult / 2),\n","                                         kernel_size=3, stride=2,\n","                                         padding=1, output_padding=1,\n","                                         bias=True),\n","                      nn.InstanceNorm2d(int(ngf * mult / 2)),\n","                      nn.ReLU(True)]\n","\n","        model += [nn.ReflectionPad2d(3)]\n","        model += [nn.Conv2d(ngf, output_nc, kernel_size=7, padding=0)]\n","        model += [nn.Tanh()]\n","\n","        self.model = nn.Sequential(*model)\n","\n","    def forward(self, input): \n","        return self.model(input)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CJQ5S7QsH1bN"},"outputs":[],"source":["# Create a generator\n","netG = ResNetGenerator()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6CoeCVYH8u1"},"outputs":[],"source":["# Load a generator model that had been pretrained on the horse2zebra dataset, whose training set contains two sets of 1068 and 1335 images of horses and zebras, respectively\n","model_path = '/content/drive/MyDrive/DL_data/horse2zebra_0.4.0.pth'\n","model_data = torch.load(model_path)\n","netG.load_state_dict(model_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrBjGRwaSu5F"},"outputs":[],"source":["# Put the network in evaluation mode\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Br4GlcPiS3wh"},"outputs":[],"source":["# Define a few input transformations to make sure data enters the network with the right shape and size\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lwYEtoPsTBm-"},"outputs":[],"source":["# Read the horse image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YlSl6qkfTUl2"},"outputs":[],"source":["# Preprocessing the image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T_GWhewSTgGQ"},"outputs":[],"source":["# Convert the image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MNRp8uoa-0g0"},"outputs":[],"source":["# Generate a html file\n","!jupyter nbconvert --to html \"/content/drive/MyDrive/DL_lab/Lab 3: Pretrained Networks.ipynb\""]}],"metadata":{"colab":{"collapsed_sections":[],"toc_visible":true,"provenance":[],"authorship_tag":"ABX9TyO5nLCU/k1zquUdYP6jB++q"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}